{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0RhnBiOHhLXn59mcUybrO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["LangServe는 LangChain 앱을 REST API로 배포\n","\n","https://github.com/teddylee777/langserve_ollama?tab=readme-ov-file\n","\n","**<server.py>**"],"metadata":{"id":"7tM1W3IRBJ0e"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGGdFXA1BDLj"},"outputs":[],"source":["from fastapi import FastAPI\n","from fastapi.responses import RedirectResponse\n","from fastapi.middleware.cors import CORSMiddleware\n","from typing import List, Union\n","from langserve.pydantic_v1 import BaseModel, Field\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langserve import add_routes\n","from chain import chain\n","from chat import chain as chat_chain\n","from translator import chain as EN_TO_KO_chain\n","from llm import llm as model\n","from xionic import chain as xionic_chain\n","\n","app = FastAPI()\n","\n","# Set all CORS enabled origins\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n","    expose_headers=[\"*\"],\n",")\n","\n","@app.get(\"/\")\n","async def redirect_root_to_docs():\n","    return RedirectResponse(\"/xionic/playground\")\n","\n","\n","add_routes(app, chain, path=\"/prompt\")\n","\n","\n","class InputChat(BaseModel):\n","    \"\"\"Input for the chat endpoint.\"\"\"\n","\n","    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(\n","        ...,\n","        description=\"The chat messages representing the current conversation.\",\n","    )\n","\n","add_routes(\n","    app,\n","    chat_chain.with_types(input_type=InputChat),\n","    path=\"/chat\",\n","    enable_feedback_endpoint=True,\n","    enable_public_trace_link_endpoint=True,\n","    playground_type=\"chat\",\n",")\n","\n","add_routes(app, EN_TO_KO_chain, path=\"/translate\")\n","\n","add_routes(app, model, path=\"/llm\")\n","\n","add_routes(\n","    app,\n","    xionic_chain.with_types(input_type=InputChat),\n","    path=\"/xionic\",\n","    enable_feedback_endpoint=True,\n","    enable_public_trace_link_endpoint=True,\n","    playground_type=\"chat\",\n",")\n","\n","if __name__ == \"__main__\":\n","    import uvicorn\n","\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"]},{"cell_type":"markdown","source":["실행\n","중요: server.py외에 langserve_ollama-main\\app 디렉토리 내 파일 전체가 server.py와 동일한 디렉토리에 있어야 함."],"metadata":{"id":"1SyaDAe2BOlL"}},{"cell_type":"code","source":["$ python server.py"],"metadata":{"id":"bqDy5M2ZBKRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# <xionic.py>\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","\n","#llm = ChatOpenAI(\n","    #base_url=\"http://sionic.chat:8001/v1\",\n","    #api_key=\"934c4bbc-c384-4bea-af82-1450d7f8128d\",\n","    #model=\"xionic-ko-llama-3-70b\",\n","#)\n","\n","llm = ChatOllama(model=\"Test1-10.8B\")\n","\n","# Prompt 설정\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are a helpful, smart, kind, and efficient AI assistant named '테디'. You always fulfill the user's requests to the best of your ability. You must generate an answer in Korean.\",\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")\n","\n","chain = prompt | llm | StrOutputParser()\n"],"metadata":{"id":"ONLC4iidBUYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["방법2 : 수동 실행"],"metadata":{"id":"KEPe0Dc8BcX8"}},{"cell_type":"code","source":["OLLAMA_HOST=192.168.35.92:11434 ollama serve OLLAMA_HOST=192.168.35.92:11434 ollama serve"],"metadata":{"id":"DvWpEWAMBZ6A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RAG\n","\n","**검색 단계(Retrieval Phase)**: 사용자의 질문이나 컨텍스트를 입력으로 받아서, 이와 관련된 외부 데이터를 검색하는 단계입니다. 이 때 검색 엔진이나 데이터베이스 등 다양한 소스에서 필요한 정보를 찾아냅니다. 검색된 데이터는 질문에 대한 답변을 생성하는데 적합하고 상세한 정보를 포함하는 것을 목표로 합니다.\n","\n","**생성 단계(Generation Phase)**: 검색된 데이터를 기반으로 LLM 모델이 사용자의 질문에 답변을 생성하는 단계입니다. 이 단계에서 모델은 검색된 정보와 기존의 지식을 결합하여, 주어진 질문에 대한 답변을 생성합니다.\n","\n","![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/961c8a41-8e92-4022-aaca-e1e87c240949/b666f8d9-31ef-46c9-a214-41f962b3ba40/image.png)\n","\n","<**실전! RAG 고급 기법 - Retriever (1),** 모두의AI, https://www.youtube.com/watch?v=J2AsmUODBak>\n","\n","- **풍부한 정보 제공**: RAG 모델은 검색을 통해 얻은 외부 데이터를 활용하여, 보다 구체적이고 풍부한 정보를 제공할 수 있습니다.\n","- **실시간 정보 반영**: 최신 데이터를 검색하여 반영함으로써, 모델이 실시간으로 변화하는 정보에 대응할 수 있습니다.\n","- **환각 방지**: 검색을 통해 실제 데이터에 기반한 답변을 생성함으로써, 환각 현상이 발생할 위험을 줄이고 정확도를 높일 수 있습니다.\n","\n","**문서 읽어들이고 요약**\n","\n","[**랭체인(langchain) + PDF 문서요약, Map-Reduce (7)**](https://teddylee777.github.io/langchain/langchain-tutorial-07/)\n","\n","https://teddylee777.github.io/langchain/langchain-tutorial-07/\n","\n","https://gongu.copyright.or.kr/gongu/wrt/wrt/view.do?wrtSn=9002094&menuNo=200030\n","\n","[운수_좋은_날.zip](https://prod-files-secure.s3.us-west-2.amazonaws.com/961c8a41-8e92-4022-aaca-e1e87c240949/8b211fe3-68e4-40bc-9d61-1f90684e5568/%EC%9A%B4%EC%88%98_%EC%A2%8B%EC%9D%80_%EB%82%A0.zip)"],"metadata":{"id":"h51D9OdWBhXk"}},{"cell_type":"code","source":["import bs4\n","from langchain_community.document_loaders import WebBaseLoader\n","\n","# 여러 개의 url 지정 가능\n","url1 = \"https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/\"\n","url2 = \"https://blog.langchain.dev/week-of-2-5-24-langchain-release-notes/\"\n","\n","loader = WebBaseLoader(\n","    web_paths=(url1, url2),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"article-header\", \"article-content\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","print(len(docs)"],"metadata":{"id":"r45qYsihBh6l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["텍스트 파일 로딩"],"metadata":{"id":"GYZlGQufBmUT"}},{"cell_type":"code","source":["from langchain_community.document_loaders import TextLoader\n","\n","loader = TextLoader('history.txt')\n","data = loader.load()\n","\n","print(type(data))\n","print(len(data))\n","print(data)\n","print(len(data[0].page_content)) #첫 번째 Document 객체의 문자열의 개수\n","print(data[0].metadata)"],"metadata":{"id":"Q0Cca2YWBm01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["디렉토리 로딩"],"metadata":{"id":"VY5VDqZ_BpUQ"}},{"cell_type":"code","source":["import os\n","from glob import glob\n","\n","files = glob(os.path.join('./', '*.txt'))\n","files\n","\n","from langchain_community.document_loaders import DirectoryLoader\n","\n","loader = DirectoryLoader(path='./', glob='*.txt')\n","\n","data = loader.load()\n","\n","print(len(pages)\n","\n","print(data[0])"],"metadata":{"id":"Nw7J3lg1BqhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**csv 파일 로딩**\n","\n","한국주택금융공사_주택금융관련_지수_20160101"],"metadata":{"id":"Oe2MeNYHB6wE"}},{"cell_type":"code","source":["from langchain_community.document_loaders.csv_loader import CSVLoader\n","\n","loader = CSVLoader(file_path='20160101.csv', encoding='cp949')\n","data = loader.load()\n","\n","print(len(data)\n","\n","print(data[0])"],"metadata":{"id":"QXCut1wGB8Ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PDF 로딩"],"metadata":{"id":"psmF0h42B-Vo"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","\n","pdf_filepath = '000660_SK_2023.pdf'\n","loader = PyPDFLoader(pdf_filepath)\n","pages = loader.load()\n","\n","print(len(pages))\n","print(pages[3])"],"metadata":{"id":"hD-RgklKB-t1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["$ pip install unstructured"],"metadata":{"id":"I7LXGwuLCAlR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.document_loaders import UnstructuredPDFLoader\n","\n","pdf_filepath = '000660_SK_2023.pdf'\n","\n","# 전체 텍스트를 단일 문서 객체로 변환\n","loader = UnstructuredPDFLoader(pdf_filepath)\n","pages = loader.load()\n","\n","print(pages)\n","print(len(pages))\n","print(pages[0].page_content[:1000])"],"metadata":{"id":"wRiA6ZebCB6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`unstructured` 라이브러리는 PDF 파일 내의 다양한 텍스트 조각(chunk)를 서로 다른 \"elements\"로 생성하고, 별도 설정을 하지 않으면 기본적으로 이러한 elements를 결합하여 하나의 텍스트 데이터로 반환"],"metadata":{"id":"qkLD0mTdCDz5"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyMuPDFLoader\n","\n","pdf_filepath = '000660_SK_2023.pdf'\n","\n","loader = PyMuPDFLoader(pdf_filepath)\n","pages = loader.load()\n","\n","print(len(pages))\n","print(pages[0].page_content)\n","print(pages[0].metadata)"],"metadata":{"id":"5Jpb3XeWCFki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["langchain_community.document_loaders 모듈의 PyMuPDFLoader 클래스는 PyMuPDF를 사용하여 PDF 파일의 페이지를 로드하고, 각 페이지를 개별 document 객체로 추출합니다. 특히 PDF 문서의 자세한 메타데이터를 추출하는 데 강점이 있음\n"],"metadata":{"id":"EFY1U_SdCH3v"}},{"cell_type":"code","source":["from langchain_community.document_loaders import OnlinePDFLoader\n","\n","# Transformers 논문을 로드\n","loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n","pages = loader.load()\n","\n","print(len(pages))\n","print(pages[0].page_content[:1000])"],"metadata":{"id":"4wZHsu8NCESs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFDirectoryLoader\n","\n","loader = PyPDFDirectoryLoader('./')\n","data = loader.load()\n","print(len(data))\n","print(data[1])"],"metadata":{"id":"l-hTJzQOCKcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://teddylee777.github.io/langchain/rag-tutorial/\n","\n","https://wikidocs.net/231364"],"metadata":{"id":"gM1V5tSHCM1w"}},{"cell_type":"markdown","source":["## 종합 예제: PDF 문서의 로드를 도와주는 PDF Loader 를 활용"],"metadata":{"id":"T95tr9SrCPDY"}},{"cell_type":"code","source":["pip install python-dotenv pypdf transformers\n","pip install -U langchain-community #필요할 수 있음."],"metadata":{"id":"aT8uxWGoCNS1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["실습 X, 입출력 설명으로 대체\n","\n","`PyPDFLoader` 사용하여 PDF를 로드하고, 각 문서가 페이지 내용과 페이지 번호를 포함한 메타데이터를 포함하는 문서의 배열생성\n","\n","**알려진 방법(최근 라이브러리로는 작동 어려움)**"],"metadata":{"id":"uui4Ly9CCVT9"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import LLMChain\n","from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","\n","# ========== ① 문서로드 ========== #\n","\n","# PDF 파일 로드\n","loader = PyPDFLoader(\"data/황순원-소나기.pdf\")\n","document = loader.load()\n","document[0].page_content[:200]\n","\n","# ========== ② 문서분할 ========== #\n","\n","# 스플리터 지정\n","text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n","    separator=\"\\n\\n\",  # 분할기준\n","    chunk_size=3000,   # 사이즈\n","    chunk_overlap=500, # 중첩 사이즈\n",")\n","\n","# 분할 실행\n","split_docs = text_splitter.split_documents(document)\n","# 총 분할된 도큐먼트 수\n","print(f'총 분할된 도큐먼트 수: {len(split_docs)}')\n","\n","# ========== ③ Map 단계 ========== #\n","\n","# Map 단계에서 처리할 프롬프트 정의\n","# 분할된 문서에 적용할 프롬프트 내용을 기입합니다.\n","# 여기서 {pages} 변수에는 분할된 문서가 차례대로 대입되니다.\n","map_template = \"\"\"다음은 문서 중 일부 내용입니다\n","{pages}\n","이 문서 목록을 기반으로 주요 내용을 요약해 주세요.\n","답변:\"\"\"\n","\n","# Map 프롬프트 완성\n","map_prompt = PromptTemplate.from_template(map_template)\n","\n","# Map에서 수행할 LLMChain 정의\n","llm = ChatOpenAI(temperature=0,\n","                 model_name='gpt-3.5-turbo-16k')\n","map_chain = LLMChain(llm=llm, prompt=map_prompt)\n","\n","# ========== ④ Reduce 단계 ========== #\n","\n","# Reduce 단계에서 처리할 프롬프트 정의\n","reduce_template = \"\"\"다음은 요약의 집합입니다:\n","{doc_summaries}\n","이것들을 바탕으로 통합된 요약을 만들어 주세요.\n","답변:\"\"\"\n","\n","# Reduce 프롬프트 완성\n","reduce_prompt = PromptTemplate.from_template(reduce_template)\n","\n","# Reduce에서 수행할 LLMChain 정의\n","reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n","\n","# 문서의 목록을 받아들여, 이를 단일 문자열로 결합하고, 이를 LLMChain에 전달합니다.\n","combine_documents_chain = StuffDocumentsChain(\n","    llm_chain=reduce_chain,\n","    document_variable_name=\"doc_summaries\" # Reduce 프롬프트에 대입되는 변수\n",")\n","\n","# Map 문서를 통합하고 순차적으로 Reduce합니다.\n","reduce_documents_chain = ReduceDocumentsChain(\n","    # 호출되는 최종 체인입니다.\n","    combine_documents_chain=combine_documents_chain,\n","    # 문서가 `StuffDocumentsChain`의 컨텍스트를 초과하는 경우\n","    collapse_documents_chain=combine_documents_chain,\n","    # 문서를 그룹화할 때의 토큰 최대 개수입니다.\n","    token_max=4000,\n",")\n","\n","# ========== ⑤ Map-Reduce 통합단계 ========== #\n","\n","# 문서들에 체인을 매핑하여 결합하고, 그 다음 결과들을 결합합니다.\n","map_reduce_chain = MapReduceDocumentsChain(\n","    # Map 체인\n","    llm_chain=map_chain,\n","    # Reduce 체인\n","    reduce_documents_chain=reduce_documents_chain,\n","    # 문서를 넣을 llm_chain의 변수 이름(map_template 에 정의된 변수명)\n","    document_variable_name=\"pages\",\n","    # 출력에서 매핑 단계의 결과를 반환합니다.\n","    return_intermediate_steps=False,\n",")\n","\n","# ========== ⑥ 실행 결과 ========== #\n","\n","# Map-Reduce 체인 실행\n","# 입력: 분할된 도큐먼트(②의 결과물)\n","result = map_reduce_chain.run(split_docs)\n","# 요약결과 출력\n","print(result)\n"],"metadata":{"id":"hKj3EZwSCSck"},"execution_count":null,"outputs":[]}]}