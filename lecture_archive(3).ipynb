{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMQ/8gMWMRwuAHv9bVHK+R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 코드 최신화, 첫 번째 시도"],"metadata":{"id":"YESagqZFCx0T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Knhuc9qLCrnk"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","from langchain_community.chat_models import ChatOllama  # ChatOllama 유지\n","from langchain.chains import LLMChain\n","from langchain.chains.summarize import load_summarize_chain  # 최신 요약 체인 사용\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# ========== ① 문서로드 ========== #\n","\n","# PDF 파일 로드\n","loader = PyPDFLoader(\"testfile.pdf\") #현진건 운수좋은날로 테스트\n","documents = loader.load()\n","print(f'첫 페이지 내용: {documents[0].page_content[:200]}')\n","\n","# ========== ② 문서분할 ========== #\n","\n","# 스플리터 지정\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=3000,   # 사이즈\n","    chunk_overlap=500, # 중첩 사이즈\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 분할기준\n",")\n","\n","# 분할 실행\n","split_docs = text_splitter.split_documents(documents)\n","print(f'총 분할된 도큐먼트 수: {len(split_docs)}')\n","\n","# ========== ③ 체인 생성 ========== #\n","\n","# ChatOllama 모델\n","llm = ChatOllama(model=\"llama3.1:latest\")\n","\n","# 요약 체인 생성\n","summarize_chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type=\"map_reduce\",  # 최신 버전에서 사용하는 요약 체인\n",")\n","\n","# ========== ④ 실행 결과 ========== #\n","\n","# 요약 체인 실행\n","result = summarize_chain.run(split_docs)\n","\n","# 요약 결과 출력\n","print(result)\n"]},{"cell_type":"markdown","source":["두 번째 시도,\n","****\n","**GPT4o :**\n","#**최신화 경고 반영**: `PyPDFLoader`를 `langchain_community.document_loaders`에서 가져오도록 수정.\n","\n","- **`run()` 대신 `invoke()` 사용**: 최신 LangChain 버전에서 `run()`이 제거되고 `invoke()`로 대체되었습니다.\n","- **`token_max` 제거**: `token_max`는 적합한 매개변수가 아니므로 제거했습니다."],"metadata":{"id":"LR8-Hx5wC29A"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_community.chat_models import ChatOllama  # ChatOllama 유지\n","from langchain_community.document_loaders import PyPDFLoader  # 최신화 반영\n","from langchain.chains import LLMChain\n","from langchain.chains.summarize import load_summarize_chain  # 최신 요약 체인 사용\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# ========== ① 문서로드 ========== #\n","\n","# PDF 파일 로드\n","loader = PyPDFLoader(\"testfile.pdf\") #현진건 운수좋은날로 테스트\n","documents = loader.load()\n","print(f'첫 페이지 내용: {documents[0].page_content[:200]}')\n","\n","# ========== ② 문서분할 ========== #\n","\n","# 스플리터 지정 - 토큰 제한을 고려해 청크 크기 조정\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,   # 모델의 1024 토큰 제한을 고려해 크기 조정\n","    chunk_overlap=200, # 중복 허용 범위 설정\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 분할 기준\n",")\n","\n","# 분할 실행\n","split_docs = text_splitter.split_documents(documents)\n","print(f'총 분할된 도큐먼트 수: {len(split_docs)}')\n","\n","# ========== ③ 체인 생성 ========== #\n","\n","# ChatOllama 모델\n","llm = ChatOllama(model=\"llama3.1:latest\", max_tokens=1024)  # 최대 토큰 수 설정\n","\n","# 요약 체인 생성\n","summarize_chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type=\"map_reduce\",  # 최신 버전에서 사용하는 요약 체인\n",")\n","\n","# ========== ④ 실행 결과 ========== #\n","\n","# 요약 체인 실행\n","result = summarize_chain.invoke(split_docs)  # run() 대신 invoke() 사용\n","# summarize_chain.invoke(split_docs,return_intermediate_steps=True)가 기본옵션인듯..\n","\n","# 요약 결과 출력\n","print(result)\n"],"metadata":{"id":"g1_XIyBHC7Jn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 코드 최신화 세 번째 시도"],"metadata":{"id":"D--udrcNC_q8"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_community.chat_models import ChatOllama  # ChatOllama 유지\n","from langchain_community.document_loaders import PyPDFLoader  # 최신화 반영\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema import Document\n","from langchain.chains.summarize import load_summarize_chain  # 최신 요약 체인 사용\n","from langchain.schema.runnable import RunnableMap, RunnableSequence  # 수정된 부분\n","\n","\n","\n","# ========== ① 문서로드 ========== #\n","\n","# PDF 파일 로드\n","loader = PyPDFLoader(\"testfile.pdf\") #현진건 운수좋은날로 테스트\n","documents = loader.load()\n","print(f'첫 페이지 내용: {documents[0].page_content[:200]}')\n","\n","# ========== ② 문서분할 ========== #\n","\n","# 스플리터 지정 - 토큰 제한을 고려해 청크 크기 조정\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1024,   # 모델의 1024 토큰 제한을 고려해 크기 조정\n","    chunk_overlap=200, # 중복 허용 범위 설정\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 분할 기준\n",")\n","\n","# 분할 실행\n","split_docs = text_splitter.split_documents(documents)\n","print(f'총 분할된 도큐먼트 수: {len(split_docs)}')\n","\n","# ========== ③ 체인 생성 ========== #\n","\n","# ChatOllama 모델 유지\n","llm = ChatOllama(model=\"llama3.1:latest\", max_tokens=2048)  # 최대 토큰 수 설정\n","\n","# 요약할 내용을 한국어로 요청하는 프롬프트 템플릿을 설정\n","map_template = \"\"\"다음은 문서 중 일부 내용입니다:\n","{pages}\n","이 문서 목록을 기반으로 주요 내용을 **한국어로** 요약해 주세요.\n","답변:\"\"\"\n","\n","# PromptTemplate 설정\n","map_prompt = PromptTemplate(template=map_template, input_variables=[\"pages\"])\n","\n","# ========== ④ RunnableMap 및 RunnableSequence 설정 ========== #\n","\n","# RunnableMap을 사용하여 프롬프트와 LLM 연결\n","runnable_map = RunnableMap(\n","    input={\"pages\": lambda docs: docs},  # 문서를 입력으로 사용\n","    steps={\"output\": map_prompt | llm}   # 프롬프트와 LLM을 연결\n",")\n","\n","# 요약 체인 생성 (최신 체인)\n","summarize_chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type=\"map_reduce\",  # 최신 버전에서 사용하는 요약 체인\n",")\n","\n","# ========== ⑤ 실행 결과 ========== #\n","\n","# 요약 체인 실행, 중간 단계를 포함한 결과를 반환하도록 설정\n","result = summarize_chain.invoke(split_docs, return_intermediate_steps=True)\n","\n","# 요약 결과 출력\n","print(result)"],"metadata":{"id":"nrovuAMyDCW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["웹 기반 RAG종합코드(수정됨)"],"metadata":{"id":"BbLKZ5XjDEOc"}},{"cell_type":"code","source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install transformers[torch] -U\n","\n","!pip install datasets\n","!pip install langchain\n","!pip install langchain_community\n","!pip install PyMuPDF\n","!pip install sentence-transformers\n","!pip install faiss-cpu\n","!pip install chromadb\n","!pip install transformers"],"metadata":{"id":"hRU3TrO6LWqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import warnings\n","from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain_community.chat_models import ChatOllama\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","import unicodedata\n","from tqdm import tqdm  # tqdm를 사용하여 진행 상황 표시\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","def load_files_from_directory(directory_path):\n","    \"\"\"디렉토리 내의 모든 PDF 및 TXT 파일을 로드하여 텍스트를 반환합니다.\"\"\"\n","    data = []\n","    files = [f for f in os.listdir(directory_path) if f.endswith(('.pdf', '.txt'))]\n","    for filename in tqdm(files, desc=\"Loading Files\", unit=\"file\"):\n","        file_path = os.path.join(directory_path, filename)\n","        if filename.endswith('.pdf'):\n","            loader = PyMuPDFLoader(file_path)\n","            data.extend(loader.load())\n","        elif filename.endswith('.txt'):\n","            loader = TextLoader(file_path)\n","            data.extend(loader.load())\n","    return data\n"],"metadata":{"id":"wH6jsVnXLd_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","#from langchain_community.chat_models import ChatOllama\n","\n","# 이 줄을:\n","#from langchain.llms import Ollama\n","# 다음과 같이 변경:\n","from langchain_community.llms import Ollama\n","\n","# 그리고 이 줄을:\n","#from langchain.embeddings import OllamaEmbeddings\n","# 다음과 같이 변경:\n","from langchain_community.embeddings import OllamaEmbeddings\n","\n","from langchain_core.prompts import PromptTemplate\n","\n","# BeautifulSoup 파서 설정\n","bs4.SoupStrainer(\n","    \"div\",\n","    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",")\n","\n","# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n","loader = WebBaseLoader(\n","    web_paths=(\"https://n.news.naver.com/article/437/0000378416\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            \"div\",\n","            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n","        )\n","    ),\n",")\n","\n","docs = loader.load()\n","print(f\"문서의 수: {len(docs)}\")\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n","splits = text_splitter.split_documents(docs)\n","print(f\"분할된 청크의 수: {len(splits)}\")\n","\n","# Ollama 임베딩 초기화 및 벡터스토어 생성\n","embedding_model = OllamaEmbeddings(model=\"steamdj/llama3.1-cpu-only:latest\")\n","vectorstore = FAISS.from_documents(documents=splits, embedding=embedding_model)\n","\n","# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n","retriever = vectorstore.as_retriever()\n","\n","prompt = PromptTemplate.from_template(\n","    \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n","검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n","한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n","\n","#Question:\n","{question}\n","\n","#Context:\n","{context}\n","\n","#Answer:\"\"\"\n",")\n","\n","llm = Ollama(model=\"steamdj/llama3.1-cpu-only:latest\", temperature=0.5)\n","\n","# 체인을 생성합니다.\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# 질문에 대한 답변 생성\n","question = \"부영그룹의 출산 장려 정책에 대해 설명해주세요.\"\n","answer = rag_chain.invoke(question)\n","print(answer)\n","\n","# 스트리밍 응답을 원한다면 아래 코드를 사용\n","# from langchain_teddynote.messages import stream_response\n","# answer_stream = rag_chain.stream(question)\n","# stream_response(answer_stream)"],"metadata":{"id":"zxex9EgwDFid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **<모델 파인튜닝>**\n","\n","https://medium.com/@aifusion.contact/llama-3-1-70b-gpu-requirements-5e292734d36e: llama3.1모델학습 요구사항 관련\n","\n","## **Fine-Tuning with Llama 2, Bits and Bytes, and QLoRA**\n","\n","- QLoRA: [Quantized Low Rank Adapters](https://arxiv.org/pdf/2305.14314.pdf)\n","• 소량의 양자화된 업데이트 가능한 매개변수를 사용하여 LLM(대규모 언어 모델)을 미세 조정하는 방법\n","• 소규모 매개변수를 모델에 효율적으로 추가하여 다양한 데이터 세트에서 미세 조정 가능\n","• 필요에 따라 \"어댑터\"를 모델에 교체하여 사용 가능\n","- [Bits and Bytes](https://github.com/TimDettmers/bitsandbytes):\n","• Tim Dettmers가 개발한 패키지\n","• 최적화, 행렬 곱셈, 양자화를 위한 맞춤형 CUDA 함수의 경량 래퍼 제공\n","• 모델을 가능한 한 효율적으로 로드하는 데 사용\n","- [PEFT](https://github.com/huggingface/peft):\n","• Huggingface에서 제공하는 라이브러리\n","• 비용 효율적인 미세 조정을 가능하게 하는 다양한 방법 제공\n","• Kaggle 노트북과 같은 경량 하드웨어에서 특히 유용\n","\n","https://blog.sionic.ai/finetuning_llama\n","\n","**파이썬 라이브러리 설치 (버전을 명시할 필요, 라이브러리 구성을 위해 가상환경 새로 구축 필요)**"],"metadata":{"id":"DGbxT4gGDI1a"}},{"cell_type":"code","source":["pip install bitsandbytes accelerate==0.21.0 scipy tensorboardX peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 tensorboardX"],"metadata":{"id":"bueRhuZ3DJLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CUDA Library설치**\n","\n","![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/961c8a41-8e92-4022-aaca-e1e87c240949/08aa7b13-b47e-482f-a993-9b219e033fd9/image.png)\n","\n","**CUDA Library설치**\n","\n","https://developer.nvidia.com/cuda-downloads\n","\n","Ubuntu 20.04의 경우"],"metadata":{"id":"wmCGPsODDMZw"}},{"cell_type":"code","source":["sudo apt-get install -y nvidia-open\n","\n","sudo apt-get install -y cuda-drivers\n","\n","sudo apt install nvidia-cuda-toolkit\n","\n","wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n","sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","wget https://developer.download.nvidia.com/compute/cuda/12.6.1/local_installers/cuda-repo-ubuntu2004-12-6-local_12.6.1-560.35.03-1_amd64.deb\n","sudo dpkg -i cuda-repo-ubuntu2004-12-6-local_12.6.1-560.35.03-1_amd64.deb\n","sudo cp /var/cuda-repo-ubuntu2004-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\n","sudo apt-get update\n","sudo apt-get -y install cuda-toolkit-12-6"],"metadata":{"id":"TFMVIzNaDM39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**문제점: 12.6, 560버전은 ollama 제대로 작동 안함**\n","\n","코드1"],"metadata":{"id":"mP0wBZ6vDPW8"}},{"cell_type":"code","source":["import os  # os 모듈 운영체제와 상호 작용할 수 있는 기능을 제공\n","import torch # PyTorch 라이브러리로, 주로 딥러닝과 머신러닝 모델을 구축, 학습, 테스트하는 데 사용\n","from datasets import load_dataset  # 데이터셋을 쉽게 불러오고 처리할 수 있는 기능을 제공\n","from transformers import (\n","    AutoModelForCausalLM, # 인과적 언어 추론(예: GPT)을 위한 모델을 자동으로 불러오는 클래스\n","    AutoTokenizer, # 입력 문장을 토큰 단위로 자동으로 잘라주는 역할\n","    BitsAndBytesConfig, # 모델 구성\n","    HfArgumentParser,  # 파라미터 파싱\n","    TrainingArguments,  # 훈련 설정\n","    pipeline,  # 파이프라인 설정\n","    logging,  #로깅을 위한 클래스\n",")\n","\n","# 모델 튜닝을 위한 라이브러리\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","# Hugging Face 허브에서 훈련하고자 하는 모델을 가져와서 이름 지정\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# instruction 데이터 세트 설정\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# fine-tuning(미세 조정)을 거친 후의 모델에 부여될 새로운 이름을 지정하는 변수\n","new_model = \"sionic-llama-2-7b-miniguanaco\"\n","\n","# LoRA에서 사용하는 low-rank matrices 어텐션 차원을 정의. 여기서는 64로 설정\n","# 값이 크면 클수록 더 많은 수정이 이루어지며, 모델이 더 복잡해질 수 있음\n","lora_r = 64\n","\n","# LoRA 적용 시 가중치에 곱해지는 스케일링 요소. 여기서는 16으로 설정\n","# LoRA가 적용될 때 원래 모델의 가중치에 얼마나 영향을 미칠지 결정. 높은 값은 가중치 조정의 강도를 증가시킴\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers   # LoRA 층에 적용되는 드롭아웃 확률. 여기서는 0.1 (10%)로 설정\n","lora_dropout = 0.1 # 일부 네트워크 연결을 무작위로 비활성화하여 모델의 강건함에 기여\n","\n","\n","####### bitsandbytes 파라미터 설정######\n","###BitsAndBytes:  벡터의 INT8 최대 절대값 양자화 기법을 사용(Meta AI의 라이브러리)\n","\n","# 4-bit precision 기반의 모델 로드\n","use_4bit = True\n","\n","# 4비트 기반 모델에 대한 dtype 계산\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# 양자화 유형(fp4 또는 nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# 4비트 기 모델에 대해 중첩 양자화 활성화(이중 양자화)\n","use_nested_quant = False\n","\n","\n","######## TrainingArguments 파라미터 설정#########\n","\n","#모델이 예측한 결과와 체크포인트가 저장될 출력 디렉터리\n","output_dir = \"./results\"\n","\n","# 훈련 에포크 수\n","num_train_epochs = 1\n","\n","# fp16/bf16 학습 활성화(A100으로 bf16을 True로 설정)\n","fp16 = False\n","bf16 = False\n","\n","# 훈련용 배치 크기\n","per_device_train_batch_size = 1\n","\n","# 평가용 배치 크기\n","per_device_eval_batch_size = 1\n","\n","# 그래디언트를 누적할 업데이트 스텝 횟수\n","gradient_accumulation_steps = 1\n","\n","# 그래디언트 체크포인트 활성화\n","gradient_checkpointing = True\n","\n","\n","# 그래디언트 클리핑을 위한 최대 그래디언트 노름을 설정.\n","# 그래디언트 클리핑은 그래디언트의 크기를 제한하여 훈련 중 안정성을 높임.\n","# Maximum gradient normal (그래디언트 클리핑) 0.3으로 설정\n","max_grad_norm = 0.3\n","\n","# 초기 학습률 AdamW 옵티마이저\n","learning_rate = 2e-6\n","\n","# bias/LayerNorm 가중치를 제외하고 모든 레이어에 적용할 Weight decay 값\n","weight_decay = 0.001\n","\n","# 옵티마이저 설정\n","optim = \"paged_adamw_32bit\"\n","\n","# 학습률 스케줄러의 유형 설정, 여기서는 코사인 스케줄러 사용\n","lr_scheduler_type = \"cosine\"\n","\n","# 훈련 스텝 수(num_train_epochs 재정의)\n","max_steps = -1\n","\n","# (0부터 learning rate까지) 학습 초기에 학습률을 점진적으로 증가시키 linear warmup 스텝의 Ratio\n","warmup_ratio = 0.03\n","\n","# 시퀀스를 동일한 길이의 배치로 그룹화, 메모리 절약 및 훈련 속도를 높임\n","group_by_length = True\n","\n","# X 업데이트 단계마다 체크포인트 저장\n","save_steps = 0\n","\n","# 매 X 업데이트 스텝 로그\n","logging_steps = 25\n","\n","\n","##########  SFT 파라미터 값 설정###########\n","# 최대 시퀀스 길이 설정\n","max_seq_length = None\n","\n","# 동일한 입력 시퀀스에 여러 개의 짧은 예제를 넣어 효율성을 높일 수 있음\n","packing = False\n","\n","# GPU 0 전체 모델 로드\n","device_map = {\"\": 0}\n","\n","## 데이터 세트 로딩과 데이터 타입\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","# 모델 계산에 사용될 데이터 타입 결정\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,  # 모델을 4비트로 로드할지 여부를 결정\n","    bnb_4bit_quant_type=bnb_4bit_quant_type, # 양자화 유형을 설정\n","    bnb_4bit_compute_dtype=compute_dtype,  # 계산에 사용될 데이터 타입을 설정\n","    bnb_4bit_use_double_quant=use_nested_quant, # 중첩 양자화를 사용할지 여부를 결정\n",")\n","\n","\n","########## GPU 호환성 확인\n","\n","# 만약 GPU가 최소한 버전 8 이상이라면 (major >= 8) bfloat16을 지원한다고 메시지를 출력.\n","# bfloat16은 훈련 속도를 높일 수 있는 데이터 타입.\n","\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","\n","########### 베이스 모델 로딩#####\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","\n","# 동일한 batch 내에서 입력의 크기를 동일하기 위해서 사용하는 Padding Token을 End of Sequence라고 하는 Special Token으로 사용한다.\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training. Padding을 오른쪽 위치에 추가한다.\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\", # 파인튜닝할 태스크를 Optional로 지정할 수 있는데, 여기서는 CASUAL_LM을 지정하였다.\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","trainer.train()\n","\n","# 훈련이 완료된 모델을 'new_model'에 저장\n","trainer.model.save_pretrained(new_model)\n","\n","# base_model과 new_model에 저장된 LoRA 가중치를 통합하여 새로운 모델을 생성\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16\n",")\n","model = PeftModel.from_pretrained(base_model, new_model) # LoRA 가중치를 가져와 기본 모델에 통합\n","\n","model = model.merge_and_unload()\n","\n","# 사전 훈련된 토크나이저를 다시 로드\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","\n","# 토크나이저의 패딩 토큰을 종료 토큰(end-of-sentence token)과 동일하게 설정\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# 패딩을 시퀀스의 오른쪽에 적용\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"PoWm_ImwDPuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드2"],"metadata":{"id":"n-nOuUwwDWqV"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n","from peft import LoraConfig\n","from trl import SFTTrainer\n","from datasets import load_dataset\n","\n","# 1. 모델 로드\n","model_name = \"NousResearch/Llama-2-7b-hf\"\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# 2. LoRA 설정\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 3. 학습 파라미터 설정\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    optim=\"adamw_hf\",\n","    learning_rate=2e-4,\n","    fp16=True,\n","    logging_steps=100,\n","    report_to=\"tensorboard\",\n",")\n","\n","# 4. 데이터셋 로드 (THUDM/LongWriter-6k 데이터셋 사용)\n","dataset = load_dataset(\"THUDM/LongWriter-6k\", split=\"train\")\n","print(\"dataset.column_names!!!\")\n","print(dataset.column_names)\n","\n","# 데이터셋 구조 확인\n","print(\"Dataset structure:\")\n","print(dataset[0])\n","\n","# 5. 데이터셋 토크나이즈 함수 정의\n","def tokenize(element):\n","    text = str(element['messages'])\n","    return tokenizer(text, truncation=True, max_length=512)\n","\n","# 6. 데이터셋 토크나이즈\n","tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n","\n","# 데이터 콜레이터 정의\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# 7. SFTTrainer로 학습 진행\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=tokenized_dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"messages\",\n","    max_seq_length=512,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","\n","# 8. 모델 저장\n","trainer.save_model(\"./trained_model\")\n","\n","# 9. 모델 병합 후 저장\n","merged_model = model  # 필요에 따라 병합 로직을 추가\n","merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n","tokenizer.save_pretrained(\"merged_model\")\n","\n","# 10. 파인튜닝된 모델 테스트\n","def get_completion(query, model, tokenizer, max_tokens=512):\n","    # 모델과 입력 데이터를 같은 장치로 이동 (GPU가 사용 가능할 경우 CUDA로 이동)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # 입력 데이터를 해당 장치로 이동\n","    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n","\n","    # 모델로 텍스트 생성\n","    outputs = model.generate(**inputs, max_new_tokens=max_tokens)\n","\n","    # 결과를 디코딩하여 반환\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# 테스트 실행\n","result = get_completion(\n","    query=\"'엣지 태양'에 대한 미드저니 프롬프트\",\n","    model=merged_model,\n","    tokenizer=tokenizer,\n","    max_tokens=512,\n",")\n","print(result)\n"],"metadata":{"id":"36aCO3d8DVkZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine-tuned 모델의 사용"],"metadata":{"id":"CbJgNtI2DagR"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel, PeftConfig\n","\n","# LoRA 모델 설정 로드\n","config = PeftConfig.from_pretrained(\"./trained_model\")\n","\n","# 기본 모델 로드\n","base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n","\n","# LoRA 가중치 로드\n","model = PeftModel.from_pretrained(base_model, \"./trained_model\")\n","\n","# 토크나이저 로드\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","\n","# 모델을 평가 모드로 설정\n","model.eval()\n","\n","def generate_text(prompt, max_length=100):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_length=max_length,\n","            num_return_sequences=1,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.95,\n","            temperature=0.7\n","        )\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","def chat_with_model():\n","    print(\"채팅을 시작합니다. 종료하려면 'quit'를 입력하세요.\")\n","    while True:\n","        user_input = input(\"사용자: \")\n","        if user_input.lower() == 'quit':\n","            break\n","        response = generate_text(user_input)\n","        print(\"모델:\", response)\n","\n","# 대화형 채팅 시작\n","if __name__ == \"__main__\":\n","    chat_with_model()\n","\n","\n","# 서버 실행 명령 (터미널에서):\n","# uvicorn main:app --host 0.0.0.0 --port 8000"],"metadata":{"id":"e2L12drwDcqV"},"execution_count":null,"outputs":[]}]}