{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOot3rKDzyMFUrwDO8nXLww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install transformers[torch] -U\n","\n","!pip install datasets\n","!pip install langchain\n","!pip install langchain_community\n","!pip install PyMuPDF\n","!pip install sentence-transformers\n","!pip install faiss-cpu\n","!pip install chromadb\n","!pip install transformers"],"metadata":{"id":"viiPz93E1H-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VQBhFYj7NT8"},"outputs":[],"source":["!pip install transformers datasets bitsandbytes peft trl accelerate --upgrade -qqq\n","​"]},{"cell_type":"code","source":["import os\n","import unicodedata\n","\n","import torch\n","import pandas as pd\n","from tqdm import tqdm # tqdm를 사용하여 진행 상황 표시\n","import fitz  # PyMuPDF\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    pipeline,\n","    BitsAndBytesConfig,\n","    Gemma2ForCausalLM\n",")\n","from accelerate import Accelerator\n","\n","# Langchain 관련\n","from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n","from langchain_community.chat_models import ChatOllama\n","from langchain.llms import HuggingFacePipeline\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS, Chroma\n","from langchain.schema import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.schema.output_parser import StrOutputParser\n","from langchain_community.document_loaders import TextLoader\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","warnings.filterwarnings(\"ignore\") # warning 경고문 무시"],"metadata":{"id":"zQaLCVpU2GZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#네이버 뉴스 요약 데이터셋 로드\n","# dataset = load_dataset(\"/재난대비 강령\")\n","\n","def load_files_from_directory(directory_path):\n","    \"\"\"디렉토리 내의 모든 PDF 및 TXT 파일을 로드하여 텍스트를 반환합니다.\"\"\"\n","    data = []\n","    for filename in os.listdir(directory_path):\n","        file_path = os.path.join(directory_path, filename)\n","        if filename.endswith('.pdf'):\n","            loader = PyMuPDFLoader(file_path)\n","            data.extend(loader.load())\n","        elif filename.endswith('.txt'):\n","            loader = TextLoader(file_path)\n","            data.extend(loader.load())\n","    return dataset"],"metadata":{"id":"qsLIhJil75nR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 양자 모델 Qlora, granada"],"metadata":{"id":"M-VhDbHk2WmG"}},{"cell_type":"code","source":["BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","# LoRA 설정 : 양자화된 모델에서 Adaptor를 붙여서 학습할 파라미터만 따로 구성함\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# 4bit 양자화 설정 - QLoRA로 해야 함\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# 모델 로드 (양자화 )\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n","                                             quantization_config=bnb_config,\n","                                             device_map=\"auto\"\n","                                            )"],"metadata":{"id":"vKnb7YS6734N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"t8F-GPMn72xD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# llama 모델에서 결과값이 나오게 하는 코드\n","def generate_prompts(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        prompt_list.append(\n","f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>다음 글을 요약해주세요:\n","{example['document'][i]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","{example['summary'][i]}<|eot_id|>\"\"\"\n","        )\n","    return prompt_list"],"metadata":{"id":"BDxvqT3m71AJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    max_seq_length=512,\n","    args=TrainingArguments(\n","        output_dir=\"outputs\",\n","        num_train_epochs = 1,\n","        # max_steps=300,\n","        per_device_train_batch_size=24, # GPU당 24개 배치\n","        gradient_accumulation_steps=4, # gradient 반영을 4개 step 마다\n","        optim=\"paged_adamw_8bit\",\n","        warmup_steps=0.03,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=100,\n","        push_to_hub=False,\n","        report_to='none',\n","    ),\n","    peft_config=lora_config,   # LoRA 설정값\n","    formatting_func=prompts,   # 프롬프트 템플릿 함수\n",")\n","\n","trainer.train()"],"metadata":{"id":"1koKp28B7uiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["document = f\"\"\"\n","기사내용 복붙\n","\"\"\"\n","\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": f\"\"\"다음 글을 요약해주세요. 약 6문단 이상으로요. :\\n{document}\"\"\"},\n","]\n","\n","prompt = pipe.tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n",")\n","\n","outputs = pipe(\n","    prompt,\n","    do_sample=True,\n","    temperature=0.4,\n","    top_k=50,\n","    top_p=0.95,\n","    add_special_tokens=True,\n","    eos_token_id = [ # eos_token_id를 지정하지 않으면 생성 토큰 반복\n","        pipe.tokenizer.eos_token_id,\n","        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n",")\n","\n","print(outputs[0]['generated_text'][len(prompt):])"],"metadata":{"id":"-eKnrSeZ7sjV"},"execution_count":null,"outputs":[]}]}