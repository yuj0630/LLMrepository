{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYRAZsAAqXr3AmdM+ALfpI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"B3nmWfnUtznj"},"outputs":[],"source":["## 지금의 예시는 그냥 예시용이며, 실제 완성 코드는 확연히 다를 것임을 말씀드립니다.\n","## 필요한 라이브러리 임포트\n","\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","Text(PDF) Loader\n","\n","from langchain_community.document_loaders import PyMuPDFLoader\n","\n","# PyMuPDFLoader 을 이용해 PDF 파일 로드(여기는 지오맥스가 될 예정)\n","# 여기서 PDF가 아닌 Excel, txt 형식도 가능할 것으로 보이나 데이터 전처리가 필요해 보입니다.\n","\n","loader = PyMuPDFLoader(\"labor_low.pdf\")\n","pages = loader.load()\n","TextSplitter\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# 문서를 문장으로 분리\n","## 청크 크기 500, 각 청크의 50자씩 겹치도록 청크를 나눈다\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=500,\n","    chunk_overlap=50,\n",")\n","docs = text_splitter.split_documents(pages)\n","Text Vector Embedding\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","# 문장을 임베딩으로 변환하고 벡터 저장소에 저장(단어가 어떻게 나와있는지 유의)\n","\n","embeddings = HuggingFaceEmbeddings(\n","    model_name='BAAI/bge-m3',\n","    #model_kwargs={'device':'cpu'},\n","    model_kwargs={'device':'cuda'},\n","    encode_kwargs={'normalize_embeddings':True},\n",")\n","VectorStore(Chroma)\n","\n","# 벡터 저장소 경로 설정\n","## 현재 경로에 'vectorstore' 경로 생성\n","vectorstore_path = 'vectorstore'\n","os.makedirs(vectorstore_path, exist_ok=True)\n","\n","# 벡터 저장소 생성 및 저장\n","vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=vectorstore_path)\n","# 벡터스토어 데이터를 디스크에 저장\n","vectorstore.persist()\n","print(\"Vectorstore created and persisted\")\n","Model\n","\n","from langchain_community.chat_models import ChatOllama\n","\n","# Ollama 를 이용해 로컬에서 LLM 실행\n","## llama3-ko-instruct 모델 다운로드는 Ollama 사용법 참조\n","model = ChatOllama(model=\"llama-3-Korean-Bllossom-8B\", temperature=0)\n","# Retriever\n","retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n","\n","# LangChain\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","\n","# Prompt 템플릿 생성(여기가 지오맥스에서 원하는 질문을 쓰면 원하는 대답이 나오게끔 해주는 코드)\n","# Question이 사용자 측이 지오맥스 측에 하는 질문이고, prompt가 지오맥스가 내놓는 대답입니다.\n","# 여기는 대표님이 말씀하신 재난피해 대비서로 채우면 되지 않을까요?\n","\n","template = '''지오맥스의 재난대비 챗봇입니다. 모든 대답은 한국어(Korean)으로 대답해줘.\":\n","{context}\n","\n","Question: {question}\n","'''\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","def format_docs(docs):\n","    return '\\n\\n'.join([d.page_content for d in docs])\n","\n","# RAG Chain 연결\n","rag_chain = (\n","    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")\n","\n","# Chain 실행\n","query = \"재난 상황 중 지진/홍수/폭설/가뭄 등이 일어나면 어떻게 해야 하는지 알려줘.\"\n","answer = rag_chain.invoke(query)\n","\n","print(\"Query:\", query)\n","print(\"Answer:\", answer)\n","\n","# langchain을 사용한 예시이며, query가 질문, answer가 대답입닌다.\n","# 좋은 대답을 나오게 하려면 주어진 데이터에 RAG를 사용해서 어떻게 잘 조정하면 되지 않을까요?"]},{"cell_type":"markdown","source":["## 여기서 추가로 지오맥스 쪽에서는 재난 발생이 일어날 때와 대피 정보를 알고 있으니 음성 데이터 같은 것을 추가로 사용할 수 있으면 더 좋을 듯 합니다."],"metadata":{"id":"O5e5TOBSJIVh"}},{"cell_type":"code","source":["from langchain_community.chat_models import ChatOllama\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# 대화 상태를 저장하기 위한 클래스\n","class ConversationState:\n","    def __init__(self):\n","        self.history = []\n","\n","    def add_message(self, role, message):\n","        self.history.append(f\"{role}: {message}\")\n","\n","    def get_context(self):\n","        return \"\\n\".join(self.history)\n","\n","# 대화 상태 객체 생성\n","conversation_state = ConversationState()\n","\n","# 프롬프트 템플릿 생성\n","template = '''지오맥스의 재난대비 챗봇입니다. 모든 대답은 한국어(Korean)으로 대답해줘.\n","{context}\n","\n","Question: {question}\n","'''\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","def format_docs(docs):\n","    return '\\n\\n'.join([d.page_content for d in docs])\n","\n","# RAG 체인 연결\n","def run_chain(question):\n","    # 대화 문맥 업데이트\n","    context = conversation_state.get_context()\n","\n","    rag_chain = (\n","        {'context': context + \"\\n\" + format_docs(docs), 'question': question}\n","        | prompt\n","        | model\n","        | StrOutputParser()\n","    )\n","\n","    answer = rag_chain.invoke(question)\n","\n","    # 대화 상태 업데이트\n","    conversation_state.add_message(\"User\", question)\n","    conversation_state.add_message(\"Bot\", answer)\n","\n","    return answer\n","\n","# 모델 초기화\n","model = ChatOllama(model=\"llama-3-Korean-Bllossom-8B\", temperature=0)\n","\n","# 대화 예시\n","query = \"재난 상황 중 지진/홍수/폭설/가뭄 등이 일어나면 어떻게 해야 하는지 알려줘.\"\n","answer = run_chain(query)\n","\n","print(\"Query:\", query)\n","print(\"Answer:\", answer)\n","\n","# 추가 질문\n","follow_up_query = \"그 상황에서 구체적인 조치가 필요한 부분이 있나요?\"\n","follow_up_answer = run_chain(follow_up_query)\n","\n","print(\"Follow-up Query:\", follow_up_query)\n","print(\"Follow-up Answer:\", follow_up_answer)"],"metadata":{"id":"qHzv1yZJk-li"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 추가 발화와 추가 대답이 있을 경우가 있을 경우 그 경우도 만들어 줘야 합니다."],"metadata":{"id":"zW81F--rlBTT"}},{"cell_type":"code","source":[],"metadata":{"id":"TNW4r2BFlJNF"},"execution_count":null,"outputs":[]}]}